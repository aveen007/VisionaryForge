{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29286b86-d695-46c8-b6f1-2f9fa153e7bc",
   "metadata": {},
   "source": [
    "# How the algorithm works\n",
    "### label real data using a service like roboflow/label real data usingg an old model\n",
    "### put the labeled data (masks, images) in the folder temp\n",
    "### run this line to get the polygons from the masks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d09be88-6944-4271-9c3e-d10dd71f79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run masks_to_polygons.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7ce75-0034-497c-91b6-d00311ef64e2",
   "metadata": {},
   "source": [
    "### By this stage you should have three folders inside temp (images, labels, masks)\n",
    "### run the following function (split_img_label), this function will take the temp data and transform it into a data set of images and lables each having their train and validation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c83d788-656c-4f7a-b3db-fd3ea5ceda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb9fa522-d5c4-49a7-b603-641effb98f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_img_label(data_train,data_test,folder_train,folder_test):\n",
    "    \n",
    "    # os.mkdir(folder_train)\n",
    "    # os.mkdir(folder_test)\n",
    "    main_dir = \"dataset\"\n",
    "    sub_dirs = [\"images\", \"labels\"]\n",
    "    train_val_dirs = [\"train\", \"val\"]\n",
    "    \n",
    "    # Create the main dataset directory\n",
    "    os.makedirs(main_dir, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories and train/val folders\n",
    "    for sub_dir in sub_dirs:\n",
    "        # Create the folder for images and labels\n",
    "        sub_dir_path = os.path.join(main_dir, sub_dir)\n",
    "        os.makedirs(sub_dir_path, exist_ok=True)\n",
    "        \n",
    "        for train_val in train_val_dirs:\n",
    "            # Create the train and val folders inside images and labels\n",
    "            os.makedirs(os.path.join(sub_dir_path, train_val), exist_ok=True)\n",
    "\n",
    "    \n",
    "    train_ind=list(data_train.index)\n",
    "    test_ind=list(data_test.index)\n",
    "    folder_train_images=\"dataset\\\\images\\\\train\"\n",
    "    folder_train_labels=\"dataset\\\\labels\\\\train\"\n",
    "    folder_test_images=\"dataset\\\\images\\\\val\"\n",
    "    folder_test_labels=\"dataset\\\\labels\\\\val\"\n",
    "  \n",
    "    # Train folder\n",
    "    for i in tqdm(range(len(train_ind))):\n",
    "        source ='.\\\\'+ \"tmp\\\\images\" + '\\\\'  +data_train[train_ind[i]].split('/')[3]\n",
    "        destination = '.\\\\'+ folder_train_images + '\\\\'  +data_train[train_ind[i]].split('/')[3]\n",
    "        command = f'copy \"{source}\" \"{destination}\"'\n",
    "        os.system(command)\n",
    "        source ='.\\\\'+ \"tmp\\\\labels\" + '\\\\'  +data_train[train_ind[i]].split('/')[3].split('.png')[0]+'.txt'\n",
    "        destination = '.\\\\'+ folder_train_labels + '\\\\'  +data_train[train_ind[i]].split('/')[3].split('.png')[0]+'.txt'\n",
    "        command = f'copy \"{source}\" \"{destination}\"'\n",
    "        os.system(command)\n",
    "     \n",
    "# Execute the command\n",
    "#     Test folder\n",
    "    for j in tqdm(range(len(test_ind))):\n",
    "        source ='.\\\\'+ \"tmp\\\\images\" + '\\\\'  +data_test[test_ind[j]].split('/')[3]\n",
    "        destination = '.\\\\'+ folder_test_images + '\\\\'  +data_test[test_ind[j]].split('/')[3]\n",
    "        command = f'copy \"{source}\" \"{destination}\"'\n",
    "        os.system(command)\n",
    "        source ='.\\\\'+ \"tmp\\\\labels\" + '\\\\'  +data_test[test_ind[j]].split('/')[3].split('.png')[0]+'.txt'\n",
    "        destination = '.\\\\'+ folder_test_labels + '\\\\'  +data_test[test_ind[j]].split('/')[3].split('.png')[0]+'.txt'\n",
    "        command = f'copy \"{source}\" \"{destination}\"'\n",
    "        os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67141c2-a65b-4f0b-b54e-3a64d96ca07a",
   "metadata": {},
   "source": [
    "### the  following code is only responsible for creating a list of the names of the image and label files into an array and running the split function on that indexing. !Be careful of the image type as it requires a minor change to jpg if such files exists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d270c426-70de-416b-8b3f-9c7e0b0813ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 104/104 [00:02<00:00, 36.25it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 36.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "PATH_Images = './tmp/images/'\n",
    "PATH_Labels= './tmp/labels/'\n",
    "\n",
    "list_img=[img for img in os.listdir(PATH_Images) if img.endswith('.png')==True]# you can change this extension to jpg\n",
    "list_txt=[img for img in os.listdir(PATH_Labels) if img.endswith('.txt')==True]\n",
    "\n",
    "path_img=[]\n",
    "\n",
    "for i in range (len(list_img)):\n",
    "    path_img.append(PATH_Images+list_img[i])\n",
    "    \n",
    "df=pd.DataFrame(path_img)\n",
    "\n",
    "# split \n",
    "data_train, data_test, labels_train, labels_test = train_test_split(df[0], df.index, test_size=0.20, random_state=42)\n",
    "folder_train_name=\"train\"\n",
    "folder_test_name=\"test\"\n",
    "# Function split \n",
    "split_img_label(data_train,data_test,folder_train_name,folder_test_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac6ba6-5e06-4745-857b-c20108db9ff4",
   "metadata": {},
   "source": [
    "### by this stage you should hava a ready to train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419f8fda-dfa5-4265-87b2-437ff3365b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.18 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.3  Python-3.9.20 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 1050, 2048MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=yolo.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=[0], workers=8, project=None, name=train13, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\segment\\train13\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    683635  ultralytics.nn.modules.head.Segment          [1, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 285 layers, 2,943,171 parameters, 2,943,155 gradients, 10.9 GFLOPs\n",
      "\n",
      "Transferred 375/453 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8n-seg.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# в случае добучения - указать путь до предобученной модели. базовая скачается автоматически\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolo.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\engine\\model.py:802\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 802\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\engine\\trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\engine\\trainer.py:327\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_ddp(world_size)\n\u001b[1;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m nb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)  \u001b[38;5;66;03m# number of batches\u001b[39;00m\n\u001b[0;32m    330\u001b[0m nw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m*\u001b[39m nb), \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# warmup iterations\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\engine\\trainer.py:264\u001b[0m, in \u001b[0;36mBaseTrainer._setup_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp \u001b[38;5;129;01mand\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:  \u001b[38;5;66;03m# Single-GPU and DDP\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     callbacks_backup \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mdefault_callbacks\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# backup callbacks as check_amp() resets them\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mcheck_amp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    265\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mdefault_callbacks \u001b[38;5;241m=\u001b[39m callbacks_backup  \u001b[38;5;66;03m# restore callbacks\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# DDP\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\utils\\checks.py:674\u001b[0m, in \u001b[0;36mcheck_amp\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mamp_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolo11n.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mchecks passed ✅\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\utils\\checks.py:661\u001b[0m, in \u001b[0;36mcheck_amp.<locals>.amp_allclose\u001b[1;34m(m, im)\u001b[0m\n\u001b[0;32m    659\u001b[0m batch \u001b[38;5;241m=\u001b[39m [im] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m    660\u001b[0m imgsz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;28mint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m))  \u001b[38;5;66;03m# max stride P5-32 and P6-64\u001b[39;00m\n\u001b[1;32m--> 661\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdata  \u001b[38;5;66;03m# FP32 inference\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    663\u001b[0m     b \u001b[38;5;241m=\u001b[39m m(batch, imgsz\u001b[38;5;241m=\u001b[39mimgsz, device\u001b[38;5;241m=\u001b[39mdevice, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdata  \u001b[38;5;66;03m# AMP inference\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\engine\\model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\engine\\model.py:547\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m)(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\engine\\predictor.py:303\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model, verbose)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:144\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[0m\n\u001b[0;32m    142\u001b[0m model \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[1;32m--> 144\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    146\u001b[0m     kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\nn\\tasks.py:206\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Conv2):\n\u001b[0;32m    205\u001b[0m     m\u001b[38;5;241m.\u001b[39mfuse_convs()\n\u001b[1;32m--> 206\u001b[0m m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_conv_and_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update conv\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mdelattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# remove batchnorm\u001b[39;00m\n\u001b[0;32m    208\u001b[0m m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward_fuse  \u001b[38;5;66;03m# update forward\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\ultralytics\\utils\\torch_utils.py:253\u001b[0m, in \u001b[0;36mfuse_conv_and_bn\u001b[1;34m(conv, bn)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Prepare filters\u001b[39;00m\n\u001b[0;32m    252\u001b[0m w_conv \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mview(conv\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 253\u001b[0m w_bn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(bn\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdiv(torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m)))\n\u001b[0;32m    254\u001b[0m fusedconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mcopy_(torch\u001b[38;5;241m.\u001b[39mmm(w_bn, w_conv)\u001b[38;5;241m.\u001b[39mview(fusedconv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# Prepare spatial bias\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n-seg.pt')  # в случае добучения - указать путь до предобученной модели. базовая скачается автоматически\n",
    "model.train(data='yolo.yaml', epochs=50, device=[0], imgsz=640)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbbaa64-1012-46fe-8fb4-ee0d53c7d242",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "illegal target for annotation (442746295.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    torch.cuda.is_available(): True\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m illegal target for annotation\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available(): True\n",
    "torch.cuda.device_count(): 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES']: 1,2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fee41b-0f86-4d78-944a-8212f2b695fa",
   "metadata": {},
   "source": [
    "### by this stage you should have a model somewhere in the folder (runs/segment/train%d)\n",
    "### move the images that you want to annotate to the folder \"predicted_masks/images\"\n",
    "### Folder Structure Explanation\r\n",
    "\r\n",
    "To run this notebook successfully, ensure that there is a folder named `predicted_masks` in the same directory as the notebook. Inside the `predicted_masks` folder, there should be three subfolders:\r\n",
    "\r\n",
    "1. **images**: This folder contains the images for predictions.\r\n",
    "2. **masks**: This folder contains the mask predictions corresponding to the images.\r\n",
    "3. **overlay**: This folder contains overlay images (a combination of the original images and their predicted masks).\r\n",
    "\r\n",
    "The structure should look li\n",
    "\n",
    "    ./predicted_masks \n",
    "                    ├── images (put your images here)\n",
    "                    ├── masks\n",
    "                    └── overlay\n",
    "                 this:\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656618bc-b455-498e-b832-11110917791c",
   "metadata": {},
   "source": [
    "### the following code is only responsible for overlaying the mask on top of the image, it has two versions one for png images and the other for jpg, but be aware that you have to call the right version depending on what you need in the function \"*save_prediction_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f48a500-31f1-41f4-b854-b5fad50cf475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42ee5d99-c56e-435c-93f6-e8de13c78e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def overlay_mask(image_path, mask, alpha=0.8):\n",
    "    # Load the image\n",
    "    # image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = Image.open(image_path).convert(\"RGBA\")\n",
    "    # resized_image = image.resize(384,480)\n",
    "\n",
    "    # Load the mask and convert it to an array\n",
    "    mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "    mask = np.array(mask)\n",
    "\n",
    "    # Create a colored version of the mask\n",
    "    colored_mask = np.zeros((*mask.shape, 4), dtype=np.uint8)  # RGBA\n",
    "    colored_mask[:, :, 0] = 255  # Red channel (color of the mask)\n",
    "    colored_mask[:, :, 3] = mask * 255  # Alpha channel based on the mask\n",
    "\n",
    "    # Convert the colored mask to an image\n",
    "    colored_mask_image = Image.fromarray(colored_mask, 'RGBA')\n",
    "\n",
    "    # Overlay the mask on the image with specified transparency\n",
    "    overlay = Image.alpha_composite(image, colored_mask_image)\n",
    "\n",
    "    # Save the result to the specified path\n",
    "    overlay_path = \"./predicted_masks/overlay/\" + image_path.split('/')[3]\n",
    "    overlay.save(overlay_path)\n",
    "    \n",
    "    # Convert overlay to format compatible with OpenCV (BGR) for further processing (if necessary)\n",
    "    overlay_cv = cv2.cvtColor(np.array(overlay), cv2.COLOR_RGBA2BGR)\n",
    "    cv2.imwrite(overlay_path, overlay_cv)\n",
    "\n",
    "# Example usage:\n",
    "# overlay_mask('./predicted_masks/images/00435-4194690285.png', './predicted_masks/masks/00435-4194690285.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3523a0b0-5916-4b45-b9ab-396b10d5f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def overlay_mask_jpg(image_path, mask, alpha=0.5):\n",
    "    overlay_path = \"./predicted_masks/overlay/\" + image_path.split('/')[3]\n",
    "    \n",
    "    img_pil = Image.open(image_path).convert(\"RGBA\")\n",
    "    overlay = Image.new('RGBA', img_pil.size, (255, 255, 255, 0))\n",
    "    overlay_draw = ImageDraw.Draw(overlay)\n",
    "    polygon = mask.xy[0]\n",
    "    if len(polygon) >= 3:\n",
    "        overlay_draw.polygon(polygon, outline=(0, 255, 0), fill=(0, 255, 0, 127))\n",
    "\n",
    "        polygon_shapely = Polygon(polygon)\n",
    "        centroid = polygon_shapely.centroid\n",
    "        circle_radius = 5\n",
    "        left_up_point = (centroid.x - circle_radius, centroid.y - circle_radius)\n",
    "        right_down_point = (centroid.x + circle_radius, centroid.y + circle_radius)\n",
    "        overlay_draw.ellipse([left_up_point, right_down_point], fill=(255, 0, 0))\n",
    "    \n",
    "    img_pil = Image.alpha_composite(img_pil, overlay)\n",
    "\n",
    "    frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(overlay_path, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1496d5c0-2bb6-4f0e-97e1-94bf69e8884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from pixel_size.utils import get_pixel_real_size\n",
    "from weld_processing.read_mask import plot_mask_and_point,return_points_and_size\n",
    "from PIL import Image, ImageDraw\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def save_prediction_masks():\n",
    "    PATH_Images=\"./predicted_masks/images\"\n",
    "    PATH_Masks=\"./predicted_masks/masks\"\n",
    "    \n",
    "    list_img=[img for img in os.listdir(PATH_Images) if img.endswith('.png')==True]\n",
    "    model_path = Path('runs/segment/train10/weights/best.pt')  # Adjust path as needed\n",
    "    model = YOLO(model_path)\n",
    "    for i in tqdm(list_img):\n",
    "        img_path=PATH_Images+\"/\"+ i\n",
    "        # Load the model (replace 'best.pt' with your actual model file name)\n",
    "        \n",
    "        # Load an image\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "            # Resize the image\n",
    "        \n",
    "        H, W,_ = img.shape\n",
    "        results = model(img,imgsz=480)\n",
    "        \n",
    "        mask= results[0].masks.data[0].cpu().numpy() * 255\n",
    "        print(mask.shape[0]/mask.shape[1])\n",
    "        # mask = cv2.resize(mask, (W, H))\n",
    "        print(H/W)\n",
    "    \n",
    "        mask_path=PATH_Masks+'/'+ i\n",
    " \n",
    "        cv2.imwrite(mask_path, mask)\n",
    "        print(mask_path, img_path)\n",
    "        overlay_mask_jpg(img_path,results[0].masks[0])\n",
    "\n",
    "      \n",
    "\n",
    "# # res = get_pixel_real_size(img_path)\n",
    "# # print(res)\n",
    "# # top_line_len,top_coords, bot_line_len,bot_coords = return_points_and_size('./output.png')\n",
    "# # print(f'top_line_len={top_line_len,top_line_len*res[0]}mm, bot_line_len={bot_line_len,bot_line_len*res[0]}')\n",
    "\n",
    "# # plot_mask_and_point('./output.png')\n",
    "save_prediction_masks()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826089da-1250-4e1c-8417-b949ef2cf7b4",
   "metadata": {},
   "source": [
    "#### by this stage you should have your overlayed images and masks in the folder \"predicted_masks\"\n",
    "#### you can then choose to add those newly annotated data to your data set and train a new iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b2f4c0-a092-4bbe-80f8-2644889d30ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(mask_path_predicted):\n\u001b[0;32m     18\u001b[0m             shutil\u001b[38;5;241m.\u001b[39mmove(mask_path_predicted, path_masks)\n\u001b[1;32m---> 20\u001b[0m \u001b[43madd_to_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36madd_to_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m path_images_predicted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./predicted_masks/images/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m path_overlay\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./predicted_masks/overlay/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m list_img\u001b[38;5;241m=\u001b[39m[img \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(path_overlay) \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(list_img):\n\u001b[0;32m     10\u001b[0m     img_path_predicted\u001b[38;5;241m=\u001b[39mpath_images_predicted\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m i\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "def add_to_dataset():\n",
    "    path_images = './tmp/images/'\n",
    "    path_masks= './tmp/masks/'\n",
    "    path_masks_predicted='./predicted_masks/masks/'\n",
    "    path_images_predicted='./predicted_masks/images/'\n",
    "    path_overlay='./predicted_masks/overlay/'\n",
    "    list_img=[img for img in os.listdir(path_overlay) if img.endswith('.jpg')==True]\n",
    "    for i in tqdm(list_img):\n",
    "        img_path_predicted=path_images_predicted+\"/\"+ i\n",
    "        mask_path_predicted=path_masks_predicted+\"/\"+ i\n",
    "          # Move predicted image to the image directory\n",
    "        if os.path.exists(img_path_predicted):\n",
    "            shutil.move(img_path_predicted, path_images)\n",
    "        \n",
    "        # Move predicted mask to the masks directory\n",
    "        if os.path.exists(mask_path_predicted):\n",
    "            shutil.move(mask_path_predicted, path_masks)\n",
    "        \n",
    "add_to_dataset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50a3ab6e-fe82-4288-af35-cc6ba62e2309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x480 1 weld, 11.5ms\n",
      "Speed: 3.0ms preprocess, 11.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 480)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model_path = Path('runs/segment/train3/weights/best.pt')  # Adjust path as needed\n",
    "model_path = Path('runs/segment/train10/weights/best.pt')  # Adjust path as needed\n",
    "model = YOLO(model_path)\n",
    "img_path=\"./predicted_masks/images/10a.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "results=model(img)\n",
    "annotated_frame = results[0].plot()\n",
    "annotated_frame\n",
    "cv2.imshow(\"Yolov8Inference\",annotated_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6dacda8-15cc-445b-8528-954b410ac63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".git\n",
      ".gitignore\n",
      ".ipynb_checkpoints\n",
      "AI_GENERATED_DATA\n",
      "annotating welding .ipynb\n",
      "Annotations\n",
      "annotation_helper.py\n",
      "Assets\n",
      "best.pt\n",
      "coco.py\n",
      "datasets\n",
      "Demo - COTS Dataset - Multiple Objects.ipynb\n",
      "Demo - COTS Dataset - Single Object.ipynb\n",
      "Environment\n",
      "Evaluation\n",
      "LICENSE\n",
      "masks_to_polygons.py\n",
      "package-list.txt\n",
      "pixel_size\n",
      "predicted_masks\n",
      "presentation.pdf\n",
      "README.md\n",
      "Report Aveen.ipynb\n",
      "runs\n",
      "tmp\n",
      "toTrainOn\n",
      "vgg.py\n",
      "weld_processing\n",
      "yolo.py\n",
      "yolo.yaml\n",
      "yolo11n.pt\n",
      "YoloTraining.ipynb\n",
      "yolov8n-seg.pt\n",
      "yolov8n.pt\n",
      "__pycache__\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = \"./\"\n",
    "\n",
    "# List all files and folders\n",
    "items = os.listdir(folder_path)\n",
    "\n",
    "# Print all files and folders\n",
    "for item in items:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42b213-95d2-47f9-ac69-db2b091ade49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
